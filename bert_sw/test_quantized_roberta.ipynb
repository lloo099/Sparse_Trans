{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95274c2d",
   "metadata": {},
   "source": [
    "# End to End Quantization Test\n",
    "This notebook tests our quantization implementation in one place by making sure our operations yield good accuracy on the end task. We don't check for exact correctness of the output since quantization will introduce errors that we do not worry about unless they cause an accuracy drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f18278d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "from transformers import glue_compute_metrics\n",
    "import sklearn\n",
    "import math\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import RobertaForSequenceClassification, AutoTokenizer\n",
    "from transformers.data.metrics import simple_accuracy\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput, BaseModelOutputWithPoolingAndCrossAttentions\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b867574a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/jjc/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96a4974efc04dd782cbf701fe3ac2c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ed861459554c349f4ac4af9af98b8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.utils import roberta_mrpc_dataset\n",
    "dataset = roberta_mrpc_dataset()\n",
    "\n",
    "# dataset = dataset.map(encode, batched=True)\n",
    "# dataset = dataset.map(lambda examples: {'labels': examples['label']}, batched=True)\n",
    "# dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfd0b06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(layer, hidden_states, attention_mask=None):\n",
    "    '''\n",
    "    Pass in a encoder layer (which holds pretrained weights) and hidden_states input,\n",
    "    and this function performs the same operations as the layer but in a readable fashion.\n",
    "    \n",
    "    hidden_states: <bs, seqlen, dmodel>\n",
    "    '''\n",
    "    bs, seqlen, dmodel = hidden_states.size()\n",
    "    num_heads = layer.attention.self.num_attention_heads\n",
    "    dhead = layer.attention.self.attention_head_size\n",
    "    \n",
    "    # Linear transform to get multiple heads. This is a major MAC slurper.\n",
    "    # Each of these is calling an nn.Linear layer on hidden_states.\n",
    "#     query_layer = layer.attention.self.query(hidden_states) # <bs, seqlen, dmodel>\n",
    "    query_layer = torch.matmul(hidden_states, layer.attention.self.query.weight.T) + layer.attention.self.query.bias\n",
    "    key_layer = layer.attention.self.key(hidden_states)     # <bs, seqlen, dmodel>\n",
    "    value_layer = layer.attention.self.value(hidden_states) # <bs, seqlen, dmodel>\n",
    "    \n",
    "    # Reshape and transpose for multi-head\n",
    "    new_shape = (bs, seqlen, num_heads, dhead)\n",
    "    \n",
    "    query_layer = query_layer.view(new_shape)\n",
    "    value_layer = value_layer.view(new_shape)\n",
    "    key_layer = key_layer.view(new_shape)\n",
    "    \n",
    "    query_layer = query_layer.permute(0,2,1,3) # <bs, num_head, seqlen, dhead>\n",
    "    value_layer = value_layer.permute(0,2,1,3) # <bs, num_head, seqlen, dhead>\n",
    "    # Key is transposed to match dimensions of Query for matmul\n",
    "    key_layer = key_layer.permute(0,2,3,1)     # <bs, num_head, dhead, seqlen>\n",
    "    \n",
    "    # The attention main course\n",
    "    attention_scores = torch.matmul(query_layer, key_layer)\n",
    "    attention_scores /= math.sqrt(dhead)\n",
    "    \n",
    "    if attention_mask is not None:\n",
    "        # Apply the attention mask is (precomputed for all layers in RobertaModel forward() function)\n",
    "        attention_scores = attention_scores + attention_mask\n",
    "    \n",
    "    attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "    attention_probs = layer.attention.self.dropout(attention_probs)\n",
    "    \n",
    "    # Weighted sum of Values from softmax attention\n",
    "    attention_out = torch.matmul(attention_probs, value_layer)\n",
    "    \n",
    "    attention_out = attention_out.permute(0,2,1,3).contiguous()\n",
    "    attention_out = attention_out.view(bs, seqlen, dmodel)\n",
    "    \n",
    "    # It's time for one more linear transform and layer norm\n",
    "    dense_out = layer.attention.output.dense(attention_out)\n",
    "    dense_out = layer.attention.output.dropout(dense_out)\n",
    "    \n",
    "    # LayerNorm also mplements the residual connection\n",
    "    layer_out = layer.attention.output.LayerNorm(dense_out + hidden_states)\n",
    "    \n",
    "    return layer_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75352287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ffn(layer, attention_out):\n",
    "    '''\n",
    "    Pass in the feedforward layer and attention output. Returns the same result of a FF forward pass.\n",
    "    '''\n",
    "    # Layer 1\n",
    "    output = layer.intermediate.dense(attention_out)\n",
    "    output = nn.functional.gelu(output)\n",
    "    \n",
    "    # Layer 2\n",
    "    output = layer.output.dense(output)\n",
    "    output = layer.output.dropout(output)\n",
    "    output = layer.output.LayerNorm(output + attention_out)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c280f259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_stack(model, hidden_states, attention_mask):\n",
    "    for layer_module in model.roberta.encoder.layer:\n",
    "        # MHA + LayerNorm\n",
    "        attention_out = attention(layer_module, hidden_states, attention_mask)\n",
    "        ff_out = ffn(layer_module, attention_out)\n",
    "        hidden_states = ff_out\n",
    "    sequence_output = hidden_states\n",
    "    pooled_output = model.roberta.pooler(hidden_states) if model.roberta.pooler is not None else None\n",
    "    \n",
    "    return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            past_key_values=None,\n",
    "            hidden_states=None,\n",
    "            attentions=None,\n",
    "            cross_attentions=None,\n",
    "        )\n",
    "        \n",
    "def sequence_classification(model,\n",
    "                            outputs, \n",
    "                            input_ids=None,\n",
    "                            attention_mask=None,\n",
    "                            token_type_ids=None,\n",
    "                            position_ids=None,\n",
    "                            head_mask=None,\n",
    "                            inputs_embeds=None,\n",
    "                            labels=None,\n",
    "                            output_attentions=None,\n",
    "                            output_hidden_states=None,\n",
    "                            return_dict=None,):\n",
    "    \n",
    "    sequence_output = outputs[0]\n",
    "    logits = model.classifier(sequence_output)\n",
    "\n",
    "    loss = None\n",
    "    if labels is not None:\n",
    "        if model.config.problem_type is None:\n",
    "            if model.num_labels == 1:\n",
    "                model.config.problem_type = \"regression\"\n",
    "            elif model.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                model.config.problem_type = \"single_label_classification\"\n",
    "            else:\n",
    "                model.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "        if model.config.problem_type == \"regression\":\n",
    "            loss_fct = MSELoss()\n",
    "            if self.num_labels == 1:\n",
    "                loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "            else:\n",
    "                loss = loss_fct(logits, labels)\n",
    "        elif model.config.problem_type == \"single_label_classification\":\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, model.num_labels), labels.view(-1))\n",
    "        elif model.config.problem_type == \"multi_label_classification\":\n",
    "            loss_fct = BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits, labels)\n",
    "\n",
    "    if not return_dict:\n",
    "        output = (logits,) + outputs[2:]\n",
    "        return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "    return SequenceClassifierOutput(\n",
    "        loss=loss,\n",
    "        logits=logits,\n",
    "        hidden_states=outputs.hidden_states,\n",
    "        attentions=outputs.attentions,\n",
    "    )\n",
    "\n",
    "def eval_model(model, dataloader):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.eval()\n",
    "    preds = None\n",
    "\n",
    "    for i, batch in enumerate(tqdm(dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            embedding_output = model.roberta.embeddings(\n",
    "                input_ids=batch['input_ids'],\n",
    "                position_ids=None,\n",
    "                token_type_ids=None,\n",
    "                inputs_embeds=None,\n",
    "                past_key_values_length=0,\n",
    "            )\n",
    "            \n",
    "            extended_attention_mask = model.roberta.get_extended_attention_mask(batch['attention_mask'], batch['input_ids'].size(), device)\n",
    "            outputs = encoder_stack(model, embedding_output, extended_attention_mask)\n",
    "            outputs = sequence_classification(model, outputs, **batch)\n",
    "\n",
    "            outputs_gt = model(**batch)\n",
    "            \n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "            _, logits_gt = outputs_gt[:2]\n",
    "            \n",
    "            \n",
    "            assert torch.allclose(logits_gt, logits)\n",
    "            \n",
    "            loss = outputs[0]\n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            out_label_ids = batch['labels'].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, batch['labels'].detach().cpu().numpy(), axis=0)\n",
    "        if i % 10 == 0:\n",
    "    #         print(f\"loss: {loss}\")\n",
    "            pass\n",
    "\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "\n",
    "    print(f'accuracy: {simple_accuracy(preds, out_label_ids)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1678f4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m eval_model(\u001b[43mmodel\u001b[49m, dataloader)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "eval_model(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efc44a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
