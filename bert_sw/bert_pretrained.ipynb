{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25ed24e6",
   "metadata": {},
   "source": [
    "## Bert Layer Demonstration\n",
    "The purpose of this notebook is to demonstrate the computation of a single BERT layer with the least complexity and verify it's correctness with a reference implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6f73243",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06fe61e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        return_dict=True,\n",
    "    ):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "        for i, layer_module in enumerate(self.layer):\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                if use_cache:\n",
    "                    logger.warning(\n",
    "                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                    )\n",
    "                    use_cache = False\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(layer_module),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    past_key_value,\n",
    "                    output_attentions,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[-1],)\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "                if self.config.add_cross_attention:\n",
    "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81ef0863",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        self.attention = BertAttention(config)\n",
    "        self.is_decoder = config.is_decoder\n",
    "        self.add_cross_attention = config.add_cross_attention\n",
    "        if self.add_cross_attention:\n",
    "            if not self.is_decoder:\n",
    "                raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n",
    "            self.crossattention = BertAttention(config, position_embedding_type=\"absolute\")\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
    "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
    "        self_attention_outputs = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            past_key_value=self_attn_past_key_value,\n",
    "        )\n",
    "        attention_output = self_attention_outputs[0]\n",
    "\n",
    "        # if decoder, the last output is tuple of self-attn cache\n",
    "        if self.is_decoder:\n",
    "            outputs = self_attention_outputs[1:-1]\n",
    "            present_key_value = self_attention_outputs[-1]\n",
    "        else:\n",
    "            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "        cross_attn_present_key_value = None\n",
    "        if self.is_decoder and encoder_hidden_states is not None:\n",
    "            if not hasattr(self, \"crossattention\"):\n",
    "                raise ValueError(\n",
    "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers by setting `config.add_cross_attention=True`\"\n",
    "                )\n",
    "\n",
    "            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n",
    "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
    "            cross_attention_outputs = self.crossattention(\n",
    "                attention_output,\n",
    "                attention_mask,\n",
    "                head_mask,\n",
    "                encoder_hidden_states,\n",
    "                encoder_attention_mask,\n",
    "                cross_attn_past_key_value,\n",
    "                output_attentions,\n",
    "            )\n",
    "            attention_output = cross_attention_outputs[0]\n",
    "            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n",
    "\n",
    "            # add cross-attn cache to positions 3,4 of present_key_value tuple\n",
    "            cross_attn_present_key_value = cross_attention_outputs[-1]\n",
    "            present_key_value = present_key_value + cross_attn_present_key_value\n",
    "\n",
    "        layer_output = apply_chunking_to_forward(\n",
    "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
    "        )\n",
    "        outputs = (layer_output,) + outputs\n",
    "\n",
    "        # if decoder, return the attn key/values as the last output\n",
    "        if self.is_decoder:\n",
    "            outputs = outputs + (present_key_value,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def feed_forward_chunk(self, attention_output):\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcf2957e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, config, position_embedding_type=None):\n",
    "        super().__init__()\n",
    "        self.self = BertSelfAttention(config, position_embedding_type=position_embedding_type)\n",
    "        self.output = BertSelfOutput(config)\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        if len(heads) == 0:\n",
    "            return\n",
    "        heads, index = find_pruneable_heads_and_indices(\n",
    "            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
    "        )\n",
    "\n",
    "        # Prune linear layers\n",
    "        self.self.query = prune_linear_layer(self.self.query, index)\n",
    "        self.self.key = prune_linear_layer(self.self.key, index)\n",
    "        self.self.value = prune_linear_layer(self.self.value, index)\n",
    "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
    "\n",
    "        # Update hyper params and store pruned heads\n",
    "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
    "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
    "        self.pruned_heads = self.pruned_heads.union(heads)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        self_outputs = self.self(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            encoder_hidden_states,\n",
    "            encoder_attention_mask,\n",
    "            past_key_value,\n",
    "            output_attentions,\n",
    "        )\n",
    "        attention_output = self.output(self_outputs[0], hidden_states)\n",
    "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4325919",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e781aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config, position_embedding_type=None):\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
    "            raise ValueError(\n",
    "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
    "                f\"heads ({config.num_attention_heads})\"\n",
    "            )\n",
    "\n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.position_embedding_type = position_embedding_type or getattr(\n",
    "            config, \"position_embedding_type\", \"absolute\"\n",
    "        )\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            self.max_position_embeddings = config.max_position_embeddings\n",
    "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
    "\n",
    "        self.is_decoder = config.is_decoder\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "\n",
    "        # If this is instantiated as a cross-attention module, the keys\n",
    "        # and values come from an encoder; the attention mask needs to be\n",
    "        # such that the encoder's padding tokens are not attended to.\n",
    "        is_cross_attention = encoder_hidden_states is not None\n",
    "\n",
    "        if is_cross_attention and past_key_value is not None:\n",
    "            # reuse k,v, cross_attentions\n",
    "            key_layer = past_key_value[0]\n",
    "            value_layer = past_key_value[1]\n",
    "            attention_mask = encoder_attention_mask\n",
    "        elif is_cross_attention:\n",
    "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
    "            attention_mask = encoder_attention_mask\n",
    "        elif past_key_value is not None:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
    "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
    "        else:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
    "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
    "            # key/value_states (first \"if\" case)\n",
    "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
    "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
    "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
    "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
    "            past_key_value = (key_layer, value_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            seq_length = hidden_states.size()[1]\n",
    "            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
    "            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
    "            distance = position_ids_l - position_ids_r\n",
    "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
    "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
    "\n",
    "            if self.position_embedding_type == \"relative_key\":\n",
    "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores\n",
    "            elif self.position_embedding_type == \"relative_key_query\":\n",
    "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs = attention_probs * head_mask\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "        if self.is_decoder:\n",
    "            outputs = outputs + (past_key_value,)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9730a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.intermediate_act_fn = config.hidden_act\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "095d0f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ac0d36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88192fe6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58ec12d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a17d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1251ac30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be331a38fc4d44dd8cda889f71da9034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1208876f13294aac9c0f4db6abc2a1f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2be8b61b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 2052, 2293, 2000, 2131, 1037, 3014,  999,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I would love to get a degree!\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83f1662b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.3061,  0.3279, -0.0632,  ..., -0.1472,  0.2321,  0.2366],\n",
       "         [ 0.6007,  0.1747, -0.0894,  ..., -0.0491,  0.9288,  0.3304],\n",
       "         [ 0.3819, -0.5441, -0.5877,  ..., -0.5009,  0.6451, -0.1131],\n",
       "         ...,\n",
       "         [ 0.2877, -0.0906,  0.5732,  ..., -0.5325,  0.4224,  0.0844],\n",
       "         [ 0.1473, -0.0393,  0.0152,  ...,  0.7634,  0.2633, -0.8061],\n",
       "         [ 0.8592,  0.0892, -0.2073,  ...,  0.1221, -0.5490, -0.4391]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-8.2792e-01, -3.3627e-01, -4.1765e-01,  4.9562e-01,  3.3972e-01,\n",
       "         -1.3270e-01,  8.3985e-01,  2.1215e-01, -3.4717e-01, -9.9982e-01,\n",
       "         -6.4621e-02,  7.5837e-01,  9.7956e-01,  2.0794e-01,  9.2540e-01,\n",
       "         -5.6997e-01, -3.2894e-02, -6.0711e-01,  3.1845e-01, -4.1045e-01,\n",
       "          5.8091e-01,  9.9781e-01,  4.0484e-01,  2.5507e-01,  4.2626e-01,\n",
       "          8.5738e-01, -6.7987e-01,  9.1573e-01,  9.2099e-01,  6.3846e-01,\n",
       "         -6.5107e-01,  1.7394e-01, -9.8622e-01, -1.5384e-01, -4.2242e-01,\n",
       "         -9.8297e-01,  3.4082e-01, -6.7703e-01,  7.5054e-02,  1.4488e-01,\n",
       "         -8.8061e-01,  1.9102e-01,  9.9898e-01, -4.7893e-01,  5.7681e-02,\n",
       "         -3.3115e-01, -9.9999e-01,  2.1299e-01, -8.3372e-01,  4.1930e-01,\n",
       "          3.9713e-01,  2.4899e-01,  1.3156e-01,  4.2535e-01,  3.5289e-01,\n",
       "          1.8784e-02, -1.8748e-01,  4.9634e-02, -1.9096e-01, -5.4316e-01,\n",
       "         -6.3762e-01,  3.5467e-01, -5.7702e-01, -8.3414e-01,  5.5944e-01,\n",
       "          3.5015e-01, -2.0271e-02, -2.2285e-01, -4.2835e-02, -5.3976e-02,\n",
       "          8.6840e-01,  1.9733e-01,  2.8041e-02, -8.3809e-01,  1.3334e-01,\n",
       "          1.6294e-01, -5.3938e-01,  1.0000e+00, -1.6053e-01, -9.6365e-01,\n",
       "          4.8083e-01,  4.2288e-01,  4.3742e-01,  2.6363e-01,  1.7684e-01,\n",
       "         -1.0000e+00,  3.3938e-01, -7.4302e-02, -9.8840e-01,  2.2109e-01,\n",
       "          4.3589e-01, -2.4608e-01,  3.1504e-01,  5.2140e-01, -3.1722e-01,\n",
       "         -2.5452e-01, -1.8544e-01, -4.9343e-01, -1.2110e-01, -1.2193e-01,\n",
       "         -2.2380e-02, -1.3016e-01, -2.3830e-01, -3.3606e-01,  2.3132e-01,\n",
       "         -4.3825e-01, -3.4481e-01,  4.3404e-01, -5.3911e-02,  6.5776e-01,\n",
       "          3.8659e-01, -2.8408e-01,  2.8582e-01, -9.4333e-01,  5.5393e-01,\n",
       "         -2.3538e-01, -9.8178e-01, -4.9459e-01, -9.8727e-01,  5.2804e-01,\n",
       "         -2.0261e-01, -1.7349e-01,  9.3689e-01,  2.3634e-01,  3.4780e-01,\n",
       "         -1.1859e-02, -3.7769e-01, -1.0000e+00, -4.2798e-01, -2.3152e-01,\n",
       "         -1.6739e-01, -1.5160e-01, -9.6856e-01, -9.4834e-01,  5.8262e-01,\n",
       "          9.3142e-01,  1.6321e-01,  9.9824e-01, -1.7553e-01,  9.0936e-01,\n",
       "         -5.3879e-02, -3.4359e-01,  1.9521e-01, -3.9491e-01,  6.1869e-01,\n",
       "          3.9130e-02, -3.9551e-01,  1.5061e-01, -4.7245e-02, -1.4753e-01,\n",
       "         -3.6105e-01, -9.1221e-02, -2.7828e-01, -9.3184e-01, -3.6594e-01,\n",
       "          9.3461e-01, -2.6145e-01, -5.4779e-01,  3.5614e-01, -1.2397e-01,\n",
       "         -3.0071e-01,  7.1141e-01,  5.3886e-01,  3.1581e-01, -2.9979e-01,\n",
       "          3.3478e-01,  9.8155e-03,  3.9890e-01, -7.9171e-01,  1.9971e-01,\n",
       "          4.0767e-01, -3.0072e-01, -3.1253e-01, -9.7229e-01, -2.6152e-01,\n",
       "          3.3918e-01,  9.8118e-01,  6.7176e-01,  2.0251e-01,  4.7330e-01,\n",
       "         -1.2940e-01,  4.6463e-01, -9.3716e-01,  9.7389e-01, -1.3716e-01,\n",
       "          3.1245e-01,  1.2388e-03,  3.4528e-01, -7.4245e-01, -1.5481e-01,\n",
       "          6.9731e-01, -3.5014e-01, -7.3860e-01,  1.1036e-01, -4.3543e-01,\n",
       "         -3.0442e-01, -4.9991e-01,  1.7089e-01, -2.9029e-01, -3.4995e-01,\n",
       "          2.4229e-02,  9.2605e-01,  9.3128e-01,  6.5391e-01, -3.3089e-01,\n",
       "          5.2472e-01, -8.7351e-01, -4.2721e-01,  1.0481e-01,  1.1771e-01,\n",
       "          1.7657e-01,  9.9022e-01, -3.6674e-01, -8.8670e-02, -9.1387e-01,\n",
       "         -9.8158e-01, -1.8629e-01, -8.5787e-01, -5.9852e-02, -6.2518e-01,\n",
       "          4.4259e-01,  1.2863e-01,  1.8588e-01,  3.3903e-01, -9.5193e-01,\n",
       "         -7.2387e-01,  3.8432e-01, -2.5196e-01,  4.0627e-01, -2.5955e-01,\n",
       "          8.2578e-01,  5.9456e-01, -5.2659e-01,  6.0155e-01,  8.9150e-01,\n",
       "         -5.4336e-01, -6.9055e-01,  7.3857e-01, -1.9242e-01,  8.0709e-01,\n",
       "         -5.5273e-01,  9.5731e-01,  4.7295e-01,  6.0807e-01, -8.9188e-01,\n",
       "         -2.3574e-01, -8.1588e-01, -2.0975e-01, -5.7710e-03, -2.0044e-01,\n",
       "          5.6458e-01,  5.1191e-01,  3.3032e-01,  6.4361e-01, -3.7869e-01,\n",
       "          9.8923e-01, -9.2210e-01, -9.3209e-01, -7.7650e-02, -2.3197e-01,\n",
       "         -9.8303e-01,  4.1047e-01,  3.0933e-01, -2.1155e-01, -3.9807e-01,\n",
       "         -5.3999e-01, -9.4464e-01,  7.3936e-01,  7.9793e-02,  9.6547e-01,\n",
       "         -1.7872e-01, -8.1276e-01, -4.3110e-01, -9.1763e-01, -2.4237e-01,\n",
       "         -1.4106e-01,  1.0275e-01, -1.6648e-01, -9.5228e-01,  4.4947e-01,\n",
       "          4.4219e-01,  5.1080e-01, -2.3399e-01,  9.9516e-01,  9.9998e-01,\n",
       "          9.6733e-01,  8.4097e-01,  8.6347e-01, -9.9089e-01, -4.7559e-01,\n",
       "          9.9994e-01, -9.1309e-01, -1.0000e+00, -9.0158e-01, -3.4728e-01,\n",
       "          2.7667e-01, -1.0000e+00, -1.8007e-01,  8.1733e-02, -8.7548e-01,\n",
       "          1.9613e-01,  9.6855e-01,  9.8080e-01, -1.0000e+00,  6.5176e-01,\n",
       "          9.1545e-01, -5.3086e-01,  7.7543e-01, -1.8784e-01,  9.6050e-01,\n",
       "          5.2355e-01,  4.1205e-01, -1.5881e-01,  4.2201e-01, -6.8934e-01,\n",
       "         -7.3713e-01, -2.5252e-02, -4.1377e-01,  9.6279e-01,  6.7770e-02,\n",
       "         -6.3219e-01, -8.5776e-01,  2.0532e-01, -3.6572e-02, -4.0721e-01,\n",
       "         -9.5872e-01, -1.8089e-01, -2.4266e-02,  5.8814e-01,  6.0592e-02,\n",
       "          2.6586e-01, -5.5079e-01,  1.6212e-01, -3.9153e-01,  2.6217e-01,\n",
       "          6.0699e-01, -9.2256e-01, -4.7615e-01,  1.5270e-01, -4.8075e-01,\n",
       "         -1.7586e-01, -9.5007e-01,  9.4465e-01, -3.4867e-01,  1.1134e-01,\n",
       "          1.0000e+00, -2.6299e-01, -7.9214e-01,  5.2658e-01,  1.4161e-01,\n",
       "         -1.4498e-01,  1.0000e+00,  7.5561e-01, -9.6973e-01, -4.9835e-01,\n",
       "          4.8835e-01, -4.7276e-01, -4.8639e-01,  9.9672e-01, -2.4472e-01,\n",
       "         -2.4805e-01,  1.9721e-01,  9.7124e-01, -9.8313e-01,  9.3803e-01,\n",
       "         -8.7678e-01, -9.5774e-01,  9.5506e-01,  9.2101e-01, -4.4431e-01,\n",
       "         -3.7923e-01,  8.1124e-02, -3.5058e-01,  1.6803e-01, -9.3708e-01,\n",
       "          4.5979e-01,  3.4279e-01, -9.2366e-05,  8.9378e-01, -5.4649e-01,\n",
       "         -5.1470e-01,  2.6581e-01, -2.1125e-01,  2.6343e-01,  5.4742e-01,\n",
       "          4.1213e-01, -4.1383e-02,  9.8892e-02, -2.1087e-01, -4.7218e-01,\n",
       "         -9.6843e-01,  1.8518e-01,  1.0000e+00, -9.3285e-02,  2.0128e-01,\n",
       "         -3.4245e-01, -5.7188e-02, -2.4658e-01,  4.5952e-01,  4.9961e-01,\n",
       "         -2.3585e-01, -7.3941e-01,  3.3169e-01, -9.2765e-01, -9.8353e-01,\n",
       "          5.2145e-01,  1.7715e-01, -2.1622e-01,  9.9974e-01,  3.6431e-01,\n",
       "          1.1548e-01,  1.6362e-01,  8.3619e-01, -1.1043e-02,  5.0116e-01,\n",
       "          5.1186e-01,  9.6929e-01, -2.3484e-01,  4.3002e-01,  7.1207e-01,\n",
       "         -4.9063e-01, -2.6894e-01, -6.2214e-01,  1.2192e-02, -9.1540e-01,\n",
       "          1.5731e-01, -9.3232e-01,  9.5077e-01,  5.0629e-01,  3.0545e-01,\n",
       "          1.7811e-01,  1.7674e-01,  1.0000e+00, -4.2115e-01,  4.8161e-01,\n",
       "          4.3084e-02,  6.6551e-01, -9.9218e-01, -7.3046e-01, -3.5667e-01,\n",
       "          5.6800e-02, -3.0798e-01, -3.2062e-01,  1.3404e-01, -9.4801e-01,\n",
       "          2.6156e-01,  6.3942e-02, -9.7339e-01, -9.8538e-01,  2.2665e-01,\n",
       "          6.1486e-01,  3.1377e-02, -8.3489e-01, -6.9126e-01, -4.7970e-01,\n",
       "          2.0395e-01, -1.1054e-01, -9.2756e-01,  3.3389e-01, -1.8000e-01,\n",
       "          3.2483e-01, -2.0010e-01,  4.8041e-01,  3.6478e-01,  7.3090e-01,\n",
       "         -1.5496e-03, -5.4989e-02,  5.5031e-03, -7.1351e-01,  7.1125e-01,\n",
       "         -6.9153e-01, -4.7088e-01, -1.7384e-01,  1.0000e+00, -5.0265e-01,\n",
       "          5.3536e-01,  6.1356e-01,  5.8238e-01, -8.3421e-02,  1.2469e-01,\n",
       "          5.9026e-01,  1.8318e-01, -4.0238e-01, -1.9397e-01, -2.6553e-01,\n",
       "         -2.9474e-01,  4.8853e-01,  9.6057e-02,  2.7829e-01,  7.1745e-01,\n",
       "          5.8647e-01,  1.7356e-01,  7.4544e-02, -5.8840e-02,  9.9745e-01,\n",
       "         -4.5035e-02, -6.7083e-02, -3.7713e-01, -3.3874e-03, -3.3789e-01,\n",
       "         -1.6224e-02,  1.0000e+00,  2.7086e-01,  1.2655e-01, -9.8537e-01,\n",
       "         -4.2350e-01, -8.2839e-01,  9.9991e-01,  7.8518e-01, -6.2351e-01,\n",
       "          4.8178e-01,  4.0051e-01, -8.9887e-02,  6.8241e-01, -1.2281e-01,\n",
       "         -2.1799e-01,  2.3508e-01,  7.3686e-02,  9.4730e-01, -4.6517e-01,\n",
       "         -9.4997e-01, -3.9215e-01,  3.8313e-01, -9.6331e-01,  9.9537e-01,\n",
       "         -4.6786e-01, -2.2646e-01, -3.9076e-01,  2.5924e-01, -9.3311e-02,\n",
       "         -1.9335e-01, -9.7656e-01, -1.9619e-01,  1.0411e-01,  9.3947e-01,\n",
       "          1.1161e-01, -4.7881e-01, -8.2356e-01,  6.1443e-02,  2.1589e-01,\n",
       "         -5.0239e-01, -8.8534e-01,  9.5882e-01, -9.7453e-01,  5.3861e-01,\n",
       "          1.0000e+00,  2.8533e-01, -6.5089e-01,  1.2204e-01, -4.0085e-01,\n",
       "          2.5705e-01, -1.3527e-01,  5.0489e-01, -9.4780e-01, -3.9850e-01,\n",
       "         -1.2035e-01,  1.4880e-01, -6.2314e-02,  8.2270e-02,  4.4226e-01,\n",
       "          1.7139e-01, -4.1780e-01, -5.2338e-01, -1.2130e-02,  3.7166e-01,\n",
       "          6.8983e-01, -2.1682e-01, -8.5641e-02,  4.4001e-03, -4.7469e-02,\n",
       "         -8.7526e-01, -2.7261e-01, -2.4747e-01, -9.9933e-01,  5.9315e-01,\n",
       "         -1.0000e+00, -9.4354e-02, -2.2329e-01, -2.0534e-01,  7.2248e-01,\n",
       "          5.8859e-01,  2.9391e-01, -6.3353e-01, -4.7477e-01,  7.1841e-01,\n",
       "          5.8676e-01, -1.4199e-01,  8.4268e-02, -5.7499e-01,  1.5680e-01,\n",
       "         -5.5208e-02,  2.6583e-01, -1.6052e-01,  7.4992e-01, -2.1472e-01,\n",
       "          1.0000e+00,  1.2277e-01, -3.8327e-01, -9.4953e-01,  2.0212e-01,\n",
       "         -1.3648e-01,  1.0000e+00, -8.1423e-01, -9.2500e-01,  3.5808e-01,\n",
       "         -4.9381e-01, -8.1498e-01,  2.1767e-01, -3.7366e-02, -5.5222e-01,\n",
       "         -5.7102e-01,  9.1554e-01,  7.3536e-01, -5.1864e-01,  4.3075e-01,\n",
       "         -2.4667e-01, -4.1836e-01, -1.2742e-01,  2.6366e-01,  9.8355e-01,\n",
       "          2.2987e-01,  8.1958e-01,  1.1237e-01, -9.6291e-02,  9.5813e-01,\n",
       "          1.0840e-01,  1.7462e-01,  3.3306e-03,  1.0000e+00,  2.5854e-01,\n",
       "         -8.8030e-01,  3.4349e-01, -9.7160e-01, -1.8000e-01, -9.4415e-01,\n",
       "          1.9964e-01,  1.2313e-01,  8.9559e-01, -2.2345e-01,  9.2335e-01,\n",
       "          3.8446e-02, -3.3238e-02, -1.2704e-01,  2.0734e-01,  3.4002e-01,\n",
       "         -8.9939e-01, -9.7929e-01, -9.7671e-01,  4.8091e-01, -3.9827e-01,\n",
       "          1.0357e-02,  2.5249e-01, -1.9382e-02,  2.9201e-01,  4.5521e-01,\n",
       "         -1.0000e+00,  9.0152e-01,  2.5519e-01,  6.0937e-01,  9.5316e-01,\n",
       "          5.6260e-01,  2.6117e-01,  1.8648e-01, -9.7409e-01, -9.5583e-01,\n",
       "         -3.1393e-01, -1.9895e-01,  5.6751e-01,  5.7207e-01,  8.6041e-01,\n",
       "          2.9778e-01, -4.6632e-01, -3.8935e-01, -1.4232e-01, -8.3814e-01,\n",
       "         -9.9056e-01,  3.0074e-01, -1.0108e-01, -9.3447e-01,  9.4062e-01,\n",
       "         -3.0178e-01, -9.6051e-02,  2.2698e-01, -4.5910e-01,  8.8019e-01,\n",
       "          6.2601e-01,  2.7259e-01,  1.0844e-01,  2.6084e-01,  8.1727e-01,\n",
       "          8.8463e-01,  9.7440e-01, -4.2581e-01,  6.6065e-01, -2.0832e-01,\n",
       "          3.5347e-01,  6.4481e-01, -9.1771e-01,  1.0118e-01,  1.7785e-01,\n",
       "         -7.5751e-02,  1.6597e-01, -6.2334e-02, -9.3830e-01,  1.1768e-01,\n",
       "         -1.7284e-01,  4.1398e-01, -3.0897e-01,  1.2299e-01, -4.1748e-01,\n",
       "         -4.9192e-02, -5.1671e-01, -4.7066e-01,  5.7611e-01,  2.1516e-01,\n",
       "          8.7209e-01,  6.7312e-01, -5.5665e-03, -4.2036e-01, -4.2620e-02,\n",
       "         -3.2966e-01, -8.9089e-01,  8.5952e-01,  7.2861e-02, -1.9237e-02,\n",
       "          3.5673e-01, -1.0385e-01,  6.5798e-01, -2.1588e-01, -2.8896e-01,\n",
       "         -3.6792e-01, -5.9767e-01,  7.8728e-01, -2.4745e-01, -5.0297e-01,\n",
       "         -3.6812e-01,  5.6020e-01,  2.8239e-01,  9.9863e-01, -4.3871e-01,\n",
       "         -5.4504e-01, -2.8375e-01, -3.2169e-01,  3.6799e-01, -1.4997e-01,\n",
       "         -1.0000e+00,  3.3967e-01, -1.7070e-01,  3.5716e-01, -2.0470e-01,\n",
       "          5.0395e-01, -1.9908e-01, -9.5556e-01, -1.6844e-01,  3.9216e-01,\n",
       "          3.5776e-01, -5.2978e-01, -2.8027e-01,  4.7950e-01,  4.4594e-01,\n",
       "          8.1198e-01,  7.6719e-01,  2.7121e-01,  3.8201e-01,  5.6277e-01,\n",
       "         -2.5320e-01, -6.4995e-01,  9.0214e-01]], grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model(**encoded_input)\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7a0af59c0bd6fca4f6809f74f679e008be666d4a48820f22427bd640771907a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
